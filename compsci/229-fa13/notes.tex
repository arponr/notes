\input{../../preamble}
\input{preamble}

\title{Derandomising the Johnson-Lindenstrauss lemma}
\author{Aleksandar Makelov \and Arpon Raksit}
\date{\today}

\setlength{\parindent}{1em}

\begin{document}
\maketitle
\thispagestyle{fancy}

\setcounter{tocdepth}{1}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\D}{\mathcal{D}}

\section{Introduction}

The Johnson-Lindenstrauss (JL) lemma and its variants furnish us with
distributions of dimension-reducing, (probably, approximately)
norm-preserving linear maps. Such maps have found numerous uses in
algorithm design, which of course forces us to ask: how much randomess
do we need to explicitly produce such a distribution? We present a
survey of some of the work that has been done on this problem, and end
by briefly discussing our failed attempts at improving this work. But
let's begin with some terminology, and then recall what the JL lemma
says precisely.

\begin{notation}
  Throughout, $n,m$ denote positive integers and $\delta,\epsilon$
  positive reals. We denote $\ell_2$-norm by $\|{-}\|$, the unit
  $\ell_2$-sphere in $\R^n$ by $S^{n-1}$, and the set of real $m
  \times n$ matrices (i.e., linear maps $\R^n \to \R^m$) by $\R^{m
    \times n}$.
\end{notation}

\begin{definitions}
  A distribution $\D$ over $\R^{m \times n}$ is a:
  \begin{enumerate}
  \item \emph{$(\delta,\epsilon)$-JL distribution} if $\Pr_{\Phi \sim
    \D} [|\|\Phi x\|^2 - 1| > \epsilon] < \delta$ for all $x \in
    S^{n-1}$.
  \item \emph{strong JL distribution} if it is a $(\exp(-\Omega(m \min
    \{\epsilon,\epsilon^2\})),\epsilon)$-JL distribution for all
    $\epsilon > 0$.
  \end{enumerate}
  Observe that if $\D$ is a strong JL distribution, then for $\epsilon
  \le 1$ it suffices to have $m = O(\log(1/\delta)/\epsilon^2)$ for
  $\D$ to be a $(\delta,\epsilon)$-JL distribution. This bound on $m$
  is known to be optimal.
\end{definitions}

\begin{definitions}
  A \emph{Rademacher random variable} is uniform in $\{1,-1\}$. The
  \emph{independent Rademacher distribution} over $\R^{m \times n}$ is
  given by choosing independent Rademacher entries, normalised by a
  factor of $1/\sqrt{m}$.
\end{definitions}

\begin{theorem}[JL Lemma \cite{achlioptas2003, clarkson2009}]
  \label{rademacher-JL}
  The independent Rademacher distribution over $\R^{m \times n}$ is a
  strong JL distribution.
\end{theorem}

Of course it takes $mn$ bits to specify an independent Rademacher
matrix. We want to investigate how much better we can do, by
minimising the seed length in the following definition, while keeping
the asymptotically optimal embedding dimension.

\begin{definition}
  We say $G : \{0,1\}^r \to \R^{m \times n}$ is a
  \emph{$(\delta,\epsilon)$-JL generator} if
  \[
  \Pr_{s \inr \{0,1\}^r}[|\|G(s)x\|^2 - 1| > \epsilon] < \delta
  \]
  for all $x \in S^{n-1}$, i.e., if choosing a uniformly random seed
  in $\{0,1\}^r$ and applying $G$ determines a $(\delta,\epsilon)$-JL
  distribution. We call $r$ the \emph{seed length} of $G$.
\end{definition}

The following tells us probabilistically what $r$ is possible. Our
goal (which has not yet been achieved) is to achieve this bound with
an explicit generator.

\begin{proposition}
  There exists a $(\delta,\epsilon)$-JL generator $G : \{0,1\}^r \to
  \R^{m \times n}$ with $m = O(\log(1/\delta)/\epsilon^2)$ and $r =
  O(\log(n) + \log(1/\delta))$.
\end{proposition}

\begin{proof}
  According to \cite{nelson-2011} this is proven via the probabilistic
  method by thinking about pseudorandom generators, but the proof is
  not included here.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reducing independence, a warmup}

The first general idea for derandomisation is the following. As stated
in (\ref{rademacher-JL}), a canonical distribution of matrices
satisfying the properties of the JL lemma is simply a normalised
random sign matrix with independent entries. Then a natural way to
reduce the number of random bits required is to try to replace fully
independent entries with just $l$-wise independent entries, for some
suitable $l \in \N$. In this section we give a basic illustration of
this idea, based on the moment estimation sketch of \cite{alon-1996}.

\begin{proposition}
  \label{warmup-dumb}
  Let $\mathcal{D}$ the independent Rademacher distribution over
  $\R^{m \times n}$. Let $x \in S^{n-1}$. Then $\E_{\Phi \sim
    \D}[\|\Phi x\|^2] = 1$ and $\Var_{\Phi \sim \D}[\|\Phi x\|^2] \le
  2/m$.
\end{proposition}

\begin{proof}
  Let $\Phi \sim \D$. For $1 \le i \le m$ let $\Phi_i =
  \sigma_i/\sqrt{m}$ the $i$-th row of $\Phi$, with $\{\sigma_{i,j}\}$
  independent Rademachers. First note that
  \begin{equation}
    \label{average-rows}
    \textstyle{\|\Phi x\|^2 = \sum_{i=1}^m \langle \Phi_i, x \rangle^2
      = \frac{1}{m} \sum_{i=1}^m \langle \sigma_i, x \rangle^2}.
  \end{equation}
  Then we have
  \[
  \textstyle{\langle \sigma_i, x \rangle^2 = \l(\sum_{j=1}^n
    \sigma_{i,j} x_j \r)^2 = \sum_{j,k} \sigma_{i,j}\sigma_{i,k}
    x_jx_k = \sum_j \sigma_{i,j}^2 x_j^2 + \sum_{j \ne k}
    \sigma_{i,j}\sigma_{i,k} x_jx_k.}
  \]
  Since $\sigma_{i,j}^2 = 1$ and $\E_{\Phi \sim \D}[\sigma_{i,j}] =
  0$, by independence and linearity of expectation,
  \[
  \textstyle{\E[\langle \sigma_i, x \rangle^2] = \sum_j x_j^2 +
    \sum_{j \ne k} \E[\sigma_{i,j}]\E[\sigma_{i,k}] x_jx_k} = \|x\|^2
  = 1.
  \]
  Of course it follows then by (\ref{average-rows}) that $\E_{\Phi
    \sim \D}[\|\Phi x\|^2] = 1$.

  We perform a similar calculation for variance:
  \begin{align*}
  \Var[\langle \sigma_i, x \rangle^2] &= \textstyle{\E[(\langle
      \sigma_i, x \rangle^2 - 1)^2] = \E\Big[\l(\sum_{j \ne k}
      \sigma_{i,j}\sigma_{i,k} x_jx_k\r)^2\Big]} \\ &= \textstyle{\E\l[
      \sum_{j,k} \sigma_{i,j}^2\sigma_{i,k}^2x_j^2x_k^2 + \sum_{(j,k)
        \ne (j',k')}
      \sigma_{i,j}\sigma_{i,j'}\sigma_{i,k}\sigma_{i,k'}
      x_jx_{j'}x_kx_{k'}\r]} \\ &= \textstyle{2 \sum_{j \ne k}
    x_j^2x_k^2 \le 2 \|x\|^4} = 2.
  \end{align*}
  Then since $\Phi$ has independent rows these variances add, and
  again by (\ref{average-rows}) we have that $\Var_{\Phi \sim
    \D}[\|\Phi x\|^2] \le 2/m$.
\end{proof}

The key thing to notice in the above proof is that we didn't use even
close to the full power of the independence of the entries.

\begin{definition}
  Let $\D$ a distribution over $\R^{m \times n}$ with independent
  entries. An \emph{$l$-wising} of $\D$ is a distribution $\D'$ over
  $\R^{m \times n}$ such that for $\Psi \sim \D'$ we have:
  \begin{enumerate}
  \item equality in distribution $\Psi_{i,j} \sim \Phi_{i,j}$ for
    $(i,j) \in [m] \times [n]$, where $\Phi \sim \D$,
  \item the entries $\{\Psi_{i,j}\}$ are $l$-wise independent;
  \end{enumerate}
  i.e., the entries of $\D'$ are identical to $\D$, but only required
  to be $l$-wise, rather than fully, independent.
\end{definition}

\begin{proposition}
  \label{warmup-smart}
  Let $\mathcal{D'}$ a $4$-wising of the independent Rademacher
  distribution over $\R^{m \times n}$. Let $x \in S^{n-1}$. Then
  $\E_{\Phi \sim \D}[\|\Phi x\|^2] = 1$ and $\Var_{\Phi \sim
    \D}[\|\Phi x\|^2] \le 2/m$.
\end{proposition}

\begin{proof}
  What did we use independence for in the proof of
  (\ref{warmup-dumb})? We needed the expectation of a product of at
  most four distinct entries to be the product of their
  expectations. And we needed the variances of $\langle \sigma_i, x
  \rangle^2$ to add. Note for the latter we just need that $\E[\langle
    \sigma_i, x \rangle^2\langle \sigma_j, x \rangle^2] = \E[\langle
    \sigma_i, x \rangle^2]\E[\langle \sigma_j, x \rangle^2]$, which is
  again a condition on the expectation of a product of four distinct
  entries. But we don't need full independence to satisfy this, we just
  need $4$-wise independence!
\end{proof}

\begin{proposition}
  \label{warmup-result}
  There exists a $(\delta,\epsilon)$-JL generator $G : \{0,1\}^r \to
  \R^{m \times n}$ with $m = O(1/(\delta\epsilon^2))$ and $r =
  O(\log(n))$.
\end{proposition}

\begin{proof}
  Recall that we need only $O(\log(n))$ bits to specify $n^2$ $4$-wise
  independent Rademacher signs. Thus we can have $G$ generate a
  $4$-wising of the independent Rademacher distribution, and the claim
  is then immediate from (\ref{warmup-smart}) and Chebyshev's
  inequality.
\end{proof}

This illustrates that if one only relies on some moment bounds, then
one can get away with less than full independence to reduce
randomness. Of course the above gives a suboptimal embedding dimension
$m$. In the following section we present an argument from
\cite{kane-2011} which uses this same principle to almost optimally
derandomise the JL lemma.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The argument of \cite{kane-2011}}

\begin{notation}
  Here $l$ denotes a positive integer. Note all logarithms are base
  $2$, and we will often assume without loss of generality that
  various quantities are even or powers of two.
\end{notation}

\begin{definitions}
  \label{JL-moment}
  A distribution $\D$ over $\R^{m \times n}$ has the:
  \begin{enumerate}
  \item \emph{$(l,\delta,\epsilon)$-JL moment property} if $\E_{\Phi
    \sim \D} [|\|\Phi x\|^2 - 1|^l] < \epsilon^l\delta$ for all $x \in
    S^{n-1}$.
  \item \emph{strong JL moment property} if it has the $(l,
    (B\epsilon^{-1}\max \{\sqrt{l/m}, l/m\})^l, \epsilon)$-JL moment
    property for all $l \ge 2$ and $\epsilon > 0$, for some constant
    $B > 0$ (independent of $m,n,l,\epsilon$).
  \end{enumerate}
\end{definitions}

\begin{lemma}
  \label{distribution-iff-moment}
  A distribution $\D$ over $\R^{m \times n}$ is a strong JL
  distribution if and only if it has the strong JL moment property.
\end{lemma}

\begin{proof}
  Omitted. See \cite{kane-2011}.
\end{proof}

\begin{lemma}
  \label{moment-wise}
  Let $\D$ a distribution over $\R^{m \times n}$ with independent
  entries which has the $(l,\delta,\epsilon)$-JL moment
  property. Then a $2l$-wising $\D'$ of $\D$ also has the
  $(l,\delta,\epsilon)$-JL moment property.
\end{lemma}

\begin{proof}
  This is simply the observation that the $l$-th moment of
  $|\|\Phi\|^2 - 1|$ is determined only by the $2l$-wise independence
  of the entries of $\Phi \sim \D$.
\end{proof}

\begin{lemma}
  \label{lwise-gen}
  For $m \le n$, there is an explicit generator $G : \{0,1\}^r \to
  \R^{m \times n}$ of an $l$-wising of the independent Rademacher
  distribution with $r = O(l \log(n))$.
\end{lemma}

\begin{proof}
  We can specify $mn \le n^2$ $l$-wise independent random signs by
  evaluating a degree $l$ polynomial over $\mathbb{F}_{O(n^2)}$, which
  only requires $O(l \log(n^2)) = O(l \log(n))$ bits.
\end{proof}

Note (\ref{distribution-iff-moment}) gives us a way to satisfy the JL
lemma via a moment bound. As per the general idea of the previous
section, this can be used to reduce the amount of independence, and
hence randomness, we need. We first apply this directly to recover a
result of Clarkson-Woodruff.

\begin{proposition}
  \label{oneshot}
  Let $\epsilon < 1$. Let $\{\D_{m,n}\}_{m,n \in \N}$ a family of
  strong JL distributions with independent entries on $\R^{m \times
    n}$. Then for $C > 0$ sufficiently large the following holds. For
  $n \in \N$, $m \coloneqq C\log(1/\delta)/\epsilon^2$, and $l
  \coloneqq \log(1/\delta)$, a $2l$-wising $\D$ of $\D_{m,n}$ is a
  $(\delta,\epsilon)$-JL distribution.
\end{proposition}

\begin{proof}
  Let $x \in S^{n-1}$. Applying Markov's inequality,
  (\ref{distribution-iff-moment}), and (\ref{moment-wise}) gives
  \begin{equation}
    \label{moment-bound}
    \Pr_{\Phi \sim \D} [|\|\Phi x\|^2 - 1| > \epsilon] < \epsilon^{-l}
    \E_{\Phi \sim \D} [|\|\Phi x\|^2 - 1|^l] =
    (B\epsilon^{-1}\max\{\sqrt{l/m}, l/m\})^l
  \end{equation}
  Setting $C = 4B$ and plugging in our choices for $m$ and $l$
  finishes.
\end{proof}

\begin{theorem}[\cite{clarkson2009,kane-2011}]
  \label{cwcomplex}
  For $\epsilon < 1$ there exists an explicit $(\delta,\epsilon)$-JL
  generator $G : \{0,1\}^r \to \R^{m \times n}$ with $m =
  O(\log(1/\delta)/\epsilon^2))$ and $r = O(\log(n)\log(1/\delta))$.
\end{theorem}

\begin{proof}
  We choose $\D_{m,n}$ the independent Rademacher distribution in
  (\ref{oneshot}) and then apply (\ref{lwise-gen}).
\end{proof}

The way Kane-Meka-Nelson improves this is by observing that there is a
tradeoff between the embedding dimension $m$ and the moment $l$, or
equivalently the degree of independence needed, in
(\ref{moment-bound}). Thus rather than applying a single embedding, we
can iteratively embed into smaller dimensions, at each stage utilising
more and more independence.

\begin{proposition}
  \label{qbound}
  Let $\delta < 1/2$ and $\epsilon < 1$. Let $\{\D_{m,n}\}_{m,n \in
    \N}$ a family of strong JL distributions with independent entries
  on $\R^{m \times n}$. Then for $C > 0$ sufficiently large the
  following holds. For $n \in \N$, $q \in (\log^2(1,\delta),
  1/\delta)$, $m \coloneqq Cq/\epsilon^2$, and $l \coloneqq
  \Theta(\log_q(1/\delta))$, a $2l$-wising $\D$ of $\D_{m,n}$ is a
  $(\delta,\epsilon)$-JL distribution.
\end{proposition}

\begin{proof}
  We begin as in (\ref{oneshot}). For our choice of $m$ and choosing
  $C$ large enough, the right hand side of (\ref{moment-bound}) is at
  most $(l/(4q))^{b l}$ where $b \in \{1/2,1\}$. We want this to be at
  most $\delta$, so taking logarithms, we need $l$ to satisfy
  $bl(\log(q) - \log(l/4)) > \log(1/\delta)$. Let $j \coloneqq
  \log(\log_q(1/\delta))$. We claim $l \coloneqq 2^{j+2} =
  \Theta(\log_q(1/\delta))$ does the job. We have $\log(q) =
  2^{-j}\log(1/\delta)$ and $\log(l/4) = j$. Then
  \[
  q \ge \log^2(1/\delta) \implies 2^jj \le
  \frac{\log(1/\delta)}{2\log(\log(1/\delta))} \log\l(
  \frac{\log(1/\delta)}{2\log(\log(1/\delta))} \r) < \frac{1}{2}
  \log(1/\delta).
  \]
  Thus $\log(q) > 2\log(l/4)$. Now we're done since $bl \log(q) > 2
  \log(1/\delta)$.
\end{proof}

\newcommand{\final}{\text{final}}

\begin{algorithm}
  \label{iterate}
  \ \begin{enumerate}
  \item Input $(n,\delta,\epsilon)$.
  \item Define $t \coloneqq
    \log(\log(1/\delta)/\log(\log^2(1/\delta)))$, $\tilde\epsilon
    \coloneqq \epsilon/(e(t+2))$, $\tilde\delta \coloneqq
    \delta/(t+2)$.
  \item Let $C > 0$, $m_{-1} \coloneqq n$ and $m_i \coloneqq
    C\tilde\delta^{-1/2^j}/\tilde\epsilon^2$, $l_j \coloneqq
    \Theta(2^j)$ for $0 \le j \le t$, and $m_\final \coloneqq
    C\log(1/\delta)/\epsilon^2$.
  \item For $0 \le j \le t$, independently draw $\Phi_j \sim \D_j$
    with $\D_j$ a distribution over $\R^{m_j \times m_{j-1}}$ which
    has the $(l_j, \tilde\delta, \tilde\epsilon)$-JL moment property.
  \item Independently draw $\Phi_\final \sim \D_\final$ with
    $\D_\final$ a $(\tilde\delta,\tilde\epsilon)$-JL distribution over
    $\R^{m_\final \times m_t}$.
  \item Output $\Phi \coloneqq \Phi_\final \Phi_t \cdots \Phi_0$.
  \end{enumerate}
\end{algorithm}

\begin{proposition}
  \label{iterative-JL}
  Let $\D$ the distribution over $\R^{m \times n}$ defined by the
  output $\Phi$ of (\ref{iterate}). Then $m =
  O(\log(1/\delta)/\epsilon)$ and $\D$ is a $(\delta,\epsilon)$-JL
  distribution.
\end{proposition}

\begin{proof}
  Details omitted here---this just follows from our choices of
  $\tilde\delta,\tilde\epsilon$ and a union bound.
\end{proof}

\begin{theorem}[{\cite[Main]{kane-2011}}]
  \label{jelani}
  For $\delta < 1/2$, $\epsilon < 1$ there exists an explicit
  $(\delta,\epsilon)$-JL generator $G : \{0,1\}^r \to \R^{m \times n}$
  with $m = O(\log(1/\delta)/\epsilon^2))$ and
  \[
  r = O(\log(n) + \log(1/\delta)\log(\log(1/\delta)/\epsilon)).
  \]
\end{theorem}

\begin{proof}
  By (\ref{iterative-JL}) it suffices to show the number of bits
  needed to generate the output of (\ref{iterate}) is at most the
  claimed bound on $r$. We will repeatedly apply (\ref{lwise-gen}). By
  (\ref{qbound}), we can take $\D_j$ to be a $\Theta(2^j)$-wising of
  the independent Rademacher distribution. Since $\Phi_0$ maps from
  dimension $n$, it takes $O(\log(n))$ bits to generate. For $j \ge
  1$, $\Phi_j$ maps from dimension
  $C\tilde\delta^{-1/2^j}/\tilde\epsilon^2$, so it takes
  \[
  O(2^j \log(C\tilde\delta^{-1/2^j}/\tilde\epsilon^2)) = O(2^j
  \log(1/\tilde\epsilon) + \log(1/\tilde\delta)) = O(2^j
  \log(1/\epsilon) + \log(1/\delta))
  \]
  bits to generate. Summing for $1 \le j \le t$, we get the required
  number of bits
  \begin{align*}
  O\l( \textstyle{\sum_{j=1}^t (2^j \log(1/\epsilon)} +
  \log(1/\delta)) \r) &= O(2^t\log(1/\epsilon) + t \log(1/\delta))
  \\ &= O\l( \log(1/\epsilon)\log(1/\delta) +
  \log(1/\delta)\log(\log(1/\delta))\r) \\ &=
  O(\log(1/\delta)\log(\log(1/\delta)/\epsilon)).
  \end{align*}
  Finally since $m_t = O(\log^2(1/\tilde\delta)/\tilde\epsilon^2) =
  O(\log^2(1/\delta)/\epsilon^2)$, by (\ref{cwcomplex}) the number of
  bits needed to generate $\Phi_\final$ is also
  $O(\log(1/\delta)\log(\log(1/\delta)/\epsilon))$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\S}{\mathcal{S}}

\section{Spread and sample}

Another prominent technique in derandomizing JL is the use of
randomness-optimal averaging samplers. We will only give a sketch and
intuition for the methods involved.

The main idea behind averaging samplers is that the expectation of a
bounded function on a finite set $S$ can be well approximated by the
expectation of the function on a small subset drawn uniformly from a
small family of subsets of $S$. More precisely, we have the following.

\begin{theorem}[Zuckerman \cite{zuckerman1997randomness}]
  There exists a constant $C$ such that for every $\epsilon,\delta>0$
  there exists an explicit collection $\S(n,\epsilon,\delta)$ of subsets
  of $[n]$ such that:
  \begin{enumerate}
    \item $\forall S\in\S: |S|=\l((\log n +
      \log(1/\delta))/\epsilon\r)^C$.
    \item There is a $NC$ algorithm for generating $S\inr\S$ using
      $O(\log n +\log(1/\delta))$ random bits.
    \item For every $f:[n]\to[0,1]$, we have
      \[
      \Pr_{S\inr \S}\l[\l|\E_{i\inr S}f(i) -
        \E_{i\inr[n]}f(i)\r|>\epsilon\r]<\delta.
      \]
  \end{enumerate}
\end{theorem}

 This motivates the following general plan of attack:
\begin{enumerate}
\item Given a vector $w$, we apply a transformation $w\to Tw$ that
  spreads the coordinates of $w$ out but preserves $\|w\|_2$, so that
  $\|Tw\|_\infty$ becomes small (possibly only with high probability).
\item Consider a function $f$ (similar to) $i \mapsto (Tw)_i^2$, which
  is tightly bounded from above by the first step.
\item Use averaging samplers to approximate the expectation
  $\E\l[f\r]\sim \|w\|_2^2$ by a small subset of the coordinates of
  $Tw$, thus giving us a good subspace embedding of $Tw$ and hence of
  $w$.
\end{enumerate}
We shall discuss two ways in which this paradigm is applied.

\subsection{Averaging samplers composed with Euclidean sections of
  $\ell_1^n$}

This approach is due to \cite{karnin2011explicit}. Here the
transformation $T$ involves the seemingly counterintuitive step of
going to a higher dimension. The main observation is that a vector
whose $\ell_1$ norm is large compared to its $\ell_2$ norm is
well-spread: indeed, the AM-QM\footnote{Arithmetic mean-quadratic
  mean} inequality becomes an equality if and only if all the numbers
are equal, so it is reasonable to expect that if we have almost
equality the numbers have to be almost the same. Thus, $T$ can be
based on the following ``$\ell_2^n\to \ell_1^N$ embedding'' due to
Indyk.

\begin{lemma}[\cite{indyk2007uncertainty}]
  Assume $n=4^k$ for some $k\in\N$, and let $\lambda>0$. There exists
  an explicit, linear transformation $T:\R^n\to (\R^t)^d$ such that:
  \begin{enumerate}
  \item $d=n^{1+o(1)}\lambda^{-O(\log\log N)}$ and $t=O(\log\log
    n/\lambda)$.
  \item $\forall w\in\R^n:\|w\|_2=\|Tw\|_2$.
  \item For $Tw=v=(v_1,\ldots,v_d)$ with $v_i\in\R^t$, the vector
    $x=(\|v_1\|_2,\ldots,\|v_d\|_2)$ satisfies $\|x\|_1\ge
    (1-\lambda)\sqrt{d} \|x\|_2$.
  \end{enumerate}
\end{lemma}

The relationship between the $\ell_1/\ell_2$ bound and the
well-spreadness property follows easily \cite[Lemma
  III.5]{indyk2007uncertainty}. Then, one can use averaging samplers
in the way described above, to get the following.

\begin{theorem}[{\cite[Main]{karnin2011explicit}}]
  There exists a $(\delta,\epsilon)$-JL generator
  $G:\{0,1\}^r\to\R^{m\times n}$ with $r = (1+o(1))\log d +
  O(\log^2(1/(\delta\epsilon)))$.
\end{theorem}


\subsection{Averaging samplers composed with rotations}

The second approach is due to \cite{kane-2011}. Here our choice of $T$
is perhaps more intuitive: a random rotation.

\begin{notation}
  We let $H \in \{1/\sqrt{n}, -1/\sqrt{n}\}^{n \times n}$ denote the
  Hadamard matrix (assuming $n$ is a power of two), so $H$ is
  unitary. For $x \in \R^n$ let $D(x) \in \R^{n \times n}$ the
  diagonal matrix with entries given by $x$.
\end{notation}

\begin{lemma}
  \label{rotation}
  Let $x \in \{1,-1\}^n$ drawn from a $k$-wise independent
  distribution. Then for $w \in S^{n-1}$ and $0 < \alpha < 1/2$ we
  have
  \[
  \Pr[\|HD(x)w\|_\infty > n^{\alpha-1/2}] \le k^{k/2}/n^{\alpha k -1}.
  \]
\end{lemma}

What (\ref{rotation}) gives us is a suitable distribution of random
unitary transformations on $\R^n$ which spreads out coordinates. Note
that bounding $\|{-}\|_\infty$ indeed gives a notion of
well-spreadness.

As in \cite{karnin2011explicit}, this is used in conjuction with
averaging samplers to provide a dimension-reducing JL
distribution. There is one notable difference in the use of samplers
in \cite{kane-2011}, however, which is that the plan of attack
outlined above is applied \emph{iteratively}. This is in the same
spirit as the algorithm (\ref{iterate}) discussed earlier, and is
again used to embed into the asymptotically optimal dimension. This
construction ends up giving another proof of (i.e., the same bound on
seed length as) (\ref{jelani}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Things we tried}

\subsection{Expander walks}

Our first idea for improving the result (\ref{jelani}) of
\cite{kane-2011} was to use expander graphs in the process of choosing
the random seeds for each stage of the algorithm (\ref{iterate}).

The high-level intuition is that random walks on expander graphs allow
us to get almost independent uniform samples using a much smaller
number of random bits. Thus, we can try to replace the independent
seeds from the derandomization in \cite{kane-2011} with almost
independent seeds generated by an expander walk.

Expander graphs are basic pseudorandom objects that can be used to
achieve the asymptotically optimal number of random bits in many
situations \cite{hoory2006expander}.

\begin{notation}
  For a regular graph $G$, label the eigenvalues of the adjacency
  matrix by $\lambda_1\ge\ldots\ge\lambda_n$ and denote
  $\lambda(G)=\max\{|\lambda_2|,|\lambda_n|\}$.
\end{notation}

A graph $G$ is a good expander if it satisfies the below property with
small $\alpha$.

\begin{definition}
  A $c$-regular graph $G$ on $n$ vertices is called an
  \emph{$(n,c,\alpha)$-graph} if $\lambda(G)\le\alpha c$.
\end{definition}

For $n$ and $c$ fixed, one can make $\alpha$ as low as
$\Theta(1/\sqrt{c})$ for suitable, arbitrarily large $c$
\cite{morgenstern1994existence}. Of main interest to our work is the
following theorem.

\begin{theorem}[Alon, Feige, Widgerson, Zuckerman
    \cite{alon1995derandomized}]
  \label{expander-walk}
  Let $G$ be an $(n,c,\alpha)$-graph and $G_0,\ldots,G_t\subseteq V$ with
  $|G_i|\ge \beta n$. Then, if $\beta>6\alpha$, for a random walk
  $X_0,X_1,\ldots,X_t$ on $G$ started from a uniformly random vertex,
  \[
  \Pr[\forall 0\le i\le t: X_i\in G_i] \ge \beta(\beta-2\alpha)^t.
  \]
\end{theorem}

Given this, our strategy is the following: if $r$ is the maximum of
the seed length over all stages, choose an $(2^r,c,\alpha)$-graph $G$
with $\alpha=O(1/\sqrt{c})$. Each vertex of $G$ gives us a seed of
length $r$, so an $m+2$-step random walk on $G$ will give us seeds for
each stage. On each stage, there is some good set of seeds we want to
stay inside, and the probability of this is quantified by
(\ref{expander-walk}).

Unfortunately, this fails to improve the bound on the number of random
bits needed: as it turns out, the maximum seed length over all stages
is asymptotically equal to the sum of all seed lengths.

\subsection{More expanders}

One idea to fix the problem with the above would be to have a series
of unbalanced bipartite expanders with gradually increasing sizes, so
that hopefully we can get away with less than $r$ bits for our initial
vertex.

More specifically, suppose the $i$-th stage requires $r_i$ random
bits, for $0\le i\le m+1$, and let $G_i$ be a bipartite expander with
$2^{r_{i}}$ vertices on its left side $L_i$ and $2^{r_{i+1}}$ on its
right side $R_i$, and left-degree $c_i$ for $0\le i\le m$. We can
`glue' these graphs together so that the right side of $G_i$ is
identified with the left side of $G_{i+1}$. We can then start from a
uniformly random vertex in $L_0$ and do a random walk going to the
right (i.e., a single random step in each of $G_i$). However, the
total number of random bits needed for that is
\[
\log(2^{r_0}) + \sum_{i=0}^m \log(c_i) \ge \log 2^{r_0} + \sum_{i=0}^m
\log(2^{r_{i+1}-r_i}) = \log(2^{r_{m+1}}) = r_{m+1}
\]
so the number of random bits needed in this scheme is again at least
the number of bits needed the last stage.

\subsection{Almost independent random variables}

Our third idea was to relax the $k$-wise independent dsitribution $\D$
in \cite{kane-2011} to a $\lambda$-almost $t$-wise independent family
$\D'$ for some properly chosen $t\ge k$. We expected that if we come
up with a bound on $\E_{\Psi \sim \D'}[|\|\Psi x\|^2-1|^l]$ in terms
of $\E_{\Phi \sim \D}[|\|\Phi x\|^2-1|^l]$, this would allows us to
reduce the number of random bits while preserving the bounds for the
error.

However, we were not able to obtain a bound of the kind described
above, due to the possibility of negative terms appearing in the
expansion of $\E_{\Phi \sim \D}[|\|\Phi x\|^2-1|^l]$. In particular,
if the latter expectation is very small compared to $\lambda$, the
expectation $\E_{\Psi \sim \D'}[|\|\Psi x\|^2-1|^l]$ can grow very
big.

\subsection{Another possible approach}

Here's another idea which we didn't quite have time to flesh
out. We've discussed two general frameworks for reducing randomness:
reducing independence, and spreading and sampling. Some intuition for
spreading out vectors is the following. If a random sign matrix must
preserve the norm of highly concentrated vectors, then we need the
full independence of entries in order to cancel out contributions of
heavy coordinates.

This intuition suggests that there should be some natural way to
combine the two general approaches to the problem. What we wanted to
do is to try to prove the JL lemma for the independent Rademacher
distribution (\ref{rademacher-JL}) via a moment bound and first
principles as in \cite{kane-2012}. Then we wanted to see if one could
reduce the required moment in this calculation under the assumption
that the input vector is well-spread. Perhaps this could allow us to
reduce independence and randomness even further.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{amsalpha}
\bibliography{refs}

\end{document}
