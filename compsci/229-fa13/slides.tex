\documentclass[10pt]{beamer}
\usepackage{textcomp}
\usepackage{url}
\usepackage{tikz-cd}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}

\mode<presentation>

\usetheme{Boadilla}
\usecolortheme{whale}
\usefonttheme{serif}
\setbeamercovered{transparent}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\renewcommand{\P}{\operatorname*{\mathbf{Pr}}}
\newcommand{\E}{\operatorname*{\mathbf{E}}}
\renewcommand{\l}{\left}
\renewcommand{\r}{\right}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\displaystyle}
\renewcommand{\a}{\alpha}
\renewcommand{\b}{\beta}
\newcommand{\inr}{\in_{u}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Derandomizing JL]{Derandomizing the Johnson-Lindenstrauss
  transform}
\author{Arpon Raksit  \and Aleksandar Makelov}
\date{December 5, 2013}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\begin{frame}
  \sectionpage
\end{frame}

\begin{frame}{The problem}
  \begin{definition}
    For $\eps,\delta>0$, a \textbf{generator} $G:\{0,1\}^r\to \R^{s\times
      d}$ is an $(d,s,\delta,\eps)$-JL generator if for all $w\in\R^d$
    with $\|w\|=1$, we have
    \[
      \P_{y\inr\{0,1\}^r}\l[| \|(G(y))(w) \|^2 -
        1|\geq\eps\r]\leq\delta
    \]
  \end{definition}

  \pause\bigskip\bigskip

  The goal is to embed in the optimal dimension with minimal
  randomness:

  \begin{problem}
    Minimize $r$ in a $(d,O(\log(1/\delta)/\eps^2),\delta,\eps)$-JL
    generator.
  \end{problem}

\end{frame}

\begin{frame}{What is possible?}

Using the probabilistic method, one can show:
\begin{theorem}[\cite{kane2011almost, karnin2011explicit}]
  There exists an $(d,O(\log(1/\delta)/\eps^2),\delta,\eps)$-JL
  generator with seed length $r=O(\log d + \log(1/\delta))$
\end{theorem}

\bigskip\bigskip

It'd be cool to give an explicit generator that achieves this seed
length

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Spread and sample \cite{karnin2011explicit}}

\begin{frame}
  \sectionpage
\end{frame}

\begin{frame}{Motivation}
  Let's try to reduce the support size of the JL distribution.

  \pause\bigskip\bigskip

  Imagine you're a random sign matrix. A highly concentrated unit
  vector comes at you and you have to preserve its norm.

  \pause\bigskip

  You're screwed.

  \pause\bigskip\bigskip

  This motivates us to try to somehow only have to deal with ``well
  spread'' vectors.
\end{frame}

\begin{frame}{Step 1}
  Map $\R^d \to \R^D \simeq (\R^t)^b$ with $b = d^{1+o(1)}$ and $t$
  small.

  \begin{theorem}[informal]
    There exists $F : \R^d \to (\R^t)^b$ such that, for $x \in \R^d$,
    if we define $y := (\|F(x)_1\|_2,\ldots,\|F(x)_b\|_2) \in \R^b$,
    then:
    \begin{enumerate}
    \item $\|F(x)\|_2 = \|y\|_2 = \|x\|_2$
    \item $\|y\|_1$ is large compared to $\|y\|_2$, implying $y$ is
      well spread.
    \end{enumerate}
  \end{theorem}

  \pause\bigskip\bigskip
  Thus we've replaced $x$ with a well-spread $F(x)$, which moreover is
  divided into blocks whose average $\ell_2$-norm is that of $x$.
\end{frame}

\begin{frame}{Step 2}
  Randomly sample coordinates $\R^D \to \R^s$ with $s \approx
  \log(d)/(\eps \delta)$.

  \begin{theorem}
    There exists an efficiently constructible $\mathcal{T} \subseteq
    \mathcal{P}([b])$ such that each $T \in \mathcal{T}$ is of size
    $((\log(b)+\log(1/\delta))/\eps)^{O(1)}$ and $\mathcal{T}$ is
    of size $b^{O(1)}$ such that for any $f : [b] \to [0,1]$,
    \[
    \P_{T \in \mathcal{T}} [ |\E_{i \in T}f(i) - \E_{j \in
        [b]}f(j)| > \eps ] < \delta.
    \]
  \end{theorem}

  \pause\bigskip
  So we can choose a small number of coordinate projections which
  preserve norm, giving us a small distribution.

  \pause\bigskip
  At the end we embed into optimal dimension
  $\log(1/\delta)/\eps^2$ with another approach, and we end up
  with a distribution needing
  \[
  (1+o(1))d + O(\log^2(\delta^{-1}\eps^{-1}))
  \]
  random bits.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Reducing independence \cite{kane2011almost}}
\label{sec:}

\begin{frame}
  \sectionpage
\end{frame}

\begin{frame}{Simple illustration}
  Recall the AMS sketch for $F_2$ moment estimation (all the way from
  problem set 1!)

  \begin{theorem}
    $\Pi$ with random entries $\pm 1/\sqrt{s}$ satisfies gives a $1\pm
    \eps$ approximation to squared $\ell_2$-norm with probability at
    least $1 - \delta$ if $s = O(1/(\eps^2 \delta))$.
  \end{theorem}

  \pause\bigskip\bigskip
  But the proof just relied on some second and fourth moment
  calculations, so in fact we could get away with $O(1)$-wise, rather
  than fully, independent entries.

  \bigskip
  Reduced to $O(\log(d))$ random bits needed.

  \pause\bigskip\bigskip
  This sort of idea is the main technique in the following work of
  Kane-Meka-Nelson \cite{kane2011almost}
\end{frame}

\begin{frame}{Definitions}

  \begin{definition}
    A distribution $\mathcal{D}$ over $\R^{s\times d}$ is an
    \textbf{$(d,s,\delta,\eps)$-JL distribution} if for any $w\in
    S^{d-1}$, we have
    $\P_{S\sim\mathcal{D}}\l[|\|Sw\|^2-1|\geq\eps\r]\leq\delta$.
  \end{definition}

  \pause\bigskip

  \begin{definition}
    A distribution $\mathcal{D}$ over $\R^{s\times d}$ has the
    \textbf{$(d,s,t,\delta,\eps)$-JL moment property} if for any $w\in
    S^{d-1}$, we have
    $\E_{S\sim\mathcal{D}}\l[|\|Sw\|^2-1|^t\r]<\eps^t\delta$.
  \end{definition}

  \pause\bigskip

  \begin{definition}
    A distribution $\mathcal{D}$ over $\R^{s\times d}$ is a \textbf{strong
      $(d,s)$-JL distribution} if it is an
    $(d,s,\exp(-\Omega(\eps^2s)),\eps)$-JL distribution for all
    $\eps>0$. $\mathcal{D}$ has the \textbf{strong $(d,s)$-JL property} if
    it has the $(d,s,\ell,O(\sqrt{\ell/(\eps^2s)})^\ell,\eps)$-JL
    moment property for all $0<\eps<1/2$ and integers $\ell\geq2$.
  \end{definition}

\end{frame}

\begin{frame}{Moment bounds}
  \begin{theorem}
    A distribution $\mathcal{D}$ is a strong $(d,s)$-JL distribution if and
    only if it has the strong $(d,s)$-JL moment property.
  \end{theorem}

  \pause\bigskip

  In fact, the strong $(d,s)$-JL moment property applied to a
  $O(s\eps^2)=O( \log(1/\delta))$-th moment suffices to get the strong
  $(d,s)$-JL property:
  \begin{equation}
    \label{eq:main}
    \P\l[|\|Sw\|^2-1|>\eps\r]<\eps^{-\ell}
    \E_{S\sim\mathcal{D}}\l[(\|Sw\|^2-1)^\ell\r]\leq2^{O(\ell)}
    \l(\frac{1}{\eps}\sqrt{\frac{\ell}{s}}\r)^\ell
  \end{equation}

  \pause

  \begin{theorem}
    Any strong $(d,s)$-JL distribution can be derandomized with
    $O(\log(1/\delta))$-wise independent entries.
  \end{theorem}
\end{frame}

\begin{frame}{What does this give?}
  A $d\times s$ matrix for $d>s$ with $\ell$-wise independent random
  sign entries can be obtained from $O(\ell\log d)$ bits by
  polynomials over $\F_d$.

  \pause\bigskip\bigskip
  \begin{theorem}[recovering Clarkson-Woodruff
      \cite{clarkson2009numerical}] There exists an explicit
    $(d,O(\log(1/\delta)/\eps^2),\delta,\eps)$-JL generator with seed
    length $O(\log d \log (1/\delta))$.
  \end{theorem}

  \pause\bigskip\bigskip
  \cite{kane2011almost} improves this by observing that
  there is a trade-off between the amount of independence needed and
  the embedding dimension
\end{frame}

\begin{frame}[allowframebreaks]{Kane-Meka-Nelson}
  \begin{theorem}
    \label{thm:main}
    There exists a constant $C$ such that for every
    $0<\eps,\delta<1/2$, there exists an explicit
    $(d,C\log(1/\delta)/\eps^2,\delta,\eps)$-JL generator
    $G:\{0,1\}^r\to \R^{s\times d}$ with
    \[
    r=O \l(\log d + \log(1/\delta)\log \l(\frac{\log(1/\delta)}{\eps}\r)\r)
    \]
  \end{theorem}

  \begin{block}{Algorithm}
    Generic JL derandomization template:
    \begin{enumerate}
    \item Let $m:=\d\log
      \l(\frac{\log(1/\delta)}{2\log\log(1/\delta)}\r)$,
      $\eps':=\d\frac{\eps}{e(m+2)}$, $\delta':=\d\frac{\delta}{m+2}$.
    \item Let $s_i:=C/(\eps'^2\delta'^{1/2^i})$, $\ell_i:=\Theta(2^i)$
      an even integer for $i\geq0$, $s_{-1}=d$.
    \item Let $S_i$ be a random matrix drawn from a distribution with
      the $(s_{i-1},s_i,\ell_i,\delta',\eps')$-JL moment property for
      $i=0,1,\ldots,m$.
    \item Let $S_{\text{final}}$ be drawn from a $\l(s_m, O
      \l(\eps^{-2}\log(1/\delta)\r),\delta',\eps'\r)$-JL distribution.
    \item Output $S=S_{\text{final}}S_m\ldots S_0$
    \end{enumerate}
  \end{block}

  \begin{proof}[Proof of JL property]
    Let $w^i=S_i\ldots S_0w$, $w^{-1}=w$. Then, for all $0\leq i \leq
    m$,
    \[
    \P\l[| \|w^i\|^2/\|w^{i-1}\|^2-1|>\eps'\r] <
    \eps'^{-l_i}\E\l[(\|w^i\|^2/\|w^{i-1}\|^2-1)^{l_i}\r]<\delta'.
    \]
    On the last step, we have
    \[
    \P\l[|\|S_{\text{final}}w^m\|^2/\|w^m\|^2-1| > \eps'\r]<\delta'.
    \]
    Then, by a union bound, all the bad events fail to occur with
    probability at least $1-(m+2)\delta'$. That is,
    \[
    \P\l[\|S_{\text{final}}w^m\|^2 \leq (1+\eps')^{m+2}\r] \geq 1-
    (m+2)\delta'=1-\delta.
    \]
    Finally, for our choice of $\eps'$, we have
    \[
    (1+\eps')^{m+2}\leq e^{(m+2)\eps'}\leq 1+\eps. \qedhere
    \]
  \end{proof}

  \begin{proof}[Proof of seed length bound]
    \begin{enumerate}
      \item Level $0$: $O(\log d)$ bits.
      \item Level $1\leq i \leq m$: $O(l_i\log
        \l(\frac{1}{\eps'^2\delta'^{-1/2^i}}\r))$ which is never
        larger than the cost for the final level.
      \item Final level: $O(\log(1/\delta')\log(\log(1/\delta')/\eps')
        = O(\log(1/\delta)\log(\log(1/\delta)/\eps)$.
    \end{enumerate}

    \bigskip
    So the first and final steps dominate the cost.
    Notice that since we're using a union bound, we can use the same
    seed on each level.
  \end{proof}

\end{frame}

\begin{frame}{Rough ideas about \cite{kane2011almost}}

The union bound is kind of loose.

\pause\bigskip\bigskip

Maybe we can replace it with a better bound by making the random seeds
`more' independent.

\pause\bigskip\bigskip

In fact, there is a way to make the seeds almost fully independent
without wasting too many random bits!

\pause\bigskip\bigskip

We now digress to the theme of expander graphs.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Expander graphs: A love story}

\begin{frame}
  \sectionpage
\end{frame}

\begin{frame}{High-level overview}
  An expander graph is a basic `pseudorandom' object.

  \pause\bigskip\bigskip

  What this means is that an expander graph is defined explicitly but
  shares many properties with random graphs.

  \pause\bigskip\bigskip

  Consequently, expanders can be used to `fool' randomized algorithms
  and reduce the number of random bits needed.
\end{frame}

\begin{frame}{Definition}

  For simplicity, we will consider only $c$-regular expanders. There
  are several ways to define an expander $G$:
  \begin{enumerate}
  \item Combinatorially: $G$ is a good expander if every $S\subseteq
    V(G)$ has a large edge boundary relative to $c|S|$.
    \pause\bigskip\bigskip
  \item Probabilistically: $G$ is a good expander if a random walk on
    $G$ converges to the uniform distribution as fast as possible.
  \pause\bigskip\bigskip
  \item Algebraically: $G$ is a good expander if the nontrivial
    eigenvalues (the ones different from $c$) of the adjacency matrix
    of $G$ are small.
\end{enumerate}

\pause\bigskip\bigskip

Most useful to us will be the algebraic definition.

\end{frame}

\begin{frame}{Definition}
  \begin{definition}
    For a regular graph $G$, label the eigenvalues by
    $\lambda_1\geq\ldots\geq\lambda_n$ and denote
    $\lambda(G)=\max\{|\lambda_2|,|\lambda_n|\}$.
  \end{definition}

  \pause\bigskip\bigskip

  \begin{definition}
    A $c$-regular graph $G$ on $n$ vertices is called an
    $(n,c,\a)$-graph if the normalized second eigenvalue is small:
    $\lambda(G)\leq \a c$
  \end{definition}

\end{frame}

\begin{frame}[allowframebreaks]{Random walks on expanders}

  Now comes a very cool theorem that shows a way one can use expander
  graphs to sufficiently reduce the number of random bits required for
  a randomized algorithm.

  \pause\bigskip

  \begin{theorem}[Ajtai, Komlos, Szemeredi
      \cite{ajtai1987deterministic}]
    Let $G$ be an $(n,c,\a)$-graph
    and $B\subseteq V$ with $|B|=\b n$. Then, for a random walk
    $X_0,X_1,\ldots,X_t$ on $G$ started from a uniformly random
    vertex,
    \[
    \P\l[\forall 0\leq i\leq t: X_i\in B\r]\leq (\a+\b)^t.
    \]
  \end{theorem}

  \pause\bigskip

  One can deduce this fact working in an orthonormal basis of
  eigenvectors and using basic linear algebra techniques.
\end{frame}

\begin{frame}{Random walks on expanders II}

  What we need in our case however is a lower bound on the probability
  of staying in some good sets $G_0,G_1,\ldots,G_t$ each of some given
  density. This is provided by the following related result:

  \bigskip\bigskip

  \begin{theorem}[Alon, Feige, Widgerson, Zuckerman
      \cite{alon1995derandomized}] Let $G$ be an $(n,c,\a)$-graph and
    $G_0,\ldots,G_t\subseteq V$ with $|G_i|\geq \b n$. Then, if $\b>6\a$,
    for a random walk $X_0,X_1,\ldots,X_t$ on $G$ started from a
    uniformly random vertex,
    \[
    \P\l[\forall 0\leq i\leq t: X_i\in G_i\r]\geq \b(\b-2\a)^t.
    \]
  \end{theorem}

\end{frame}

\begin{frame}{How small can $\a$ be?}

We want to be able to make $\a$ as small as possible.

\pause\bigskip\bigskip

It's not hard to show that the limit is
\begin{theorem}[Alon-Boppana]
  For any $(n,c)$ graph,
  \[
  \lambda(G)\geq 2\sqrt{c-1}-o_n(1)
  \]
\end{theorem}
(and in fact a bound of $\lambda(G)\geq O(\sqrt{c})$ is
straightforward).

\pause\bigskip\bigskip

\begin{definition}
  An $(n,c)$-graph $G$ is called \textbf{Ramanujan} if
  $\lambda(G)\leq 2\sqrt{c-1}$.
\end{definition}
\end{frame}

\begin{frame}{Ramanujan graphs exist!}
  \begin{definition}
    A family of (expander) $(n,c)$-graphs is \textbf{very explicit} if
    there is an algorithm which on input $i\in\N,v\in
    V(G_i),k\in\{1,\ldots,c\}$ outputs the $k$-th neighbor of $v$ in
    $G_i$ and runs in time polynomial in $\log(i)+\log(|v|)+\log(k)$
  \end{definition}

  \pause\bigskip\bigskip

  We have the following
  \begin{theorem}
    [Lubotzky, Phillip, Sarnak, \cite{lubotzky1988ramanujan},
      Morgenstern \cite{morgenstern1994existence}] For every prime $p$
    and every $k\in\N$, there exist families of very explicit
    $p^k+1$-regular Ramanujan graphs.
  \end{theorem}

  \pause\bigskip

  Thus, we can take $\a=O \l(\frac{1}{\sqrt{c}}\r)$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Back to derandomizing JL}
\label{sec:}

\begin{frame}
  \sectionpage
\end{frame}

\begin{frame}
  Unfortunately, there are several problems with the approach we
    outlined above.

  \pause\bigskip\bigskip

  Even with full independence, the new value of $1/\delta'$ differs
  from the old by at most a polynomial in $1/\delta$, so this doesn't
  improve the number of random bits needed for the last step.

  \pause\bigskip\bigskip

  To really get full independence, it turns out we need to use
  $O\l(\log(1/\delta)
  \log\l(\frac{\log(1/\delta))}{2\log\log(1/\delta)}\r)\r)$ random
  bits, instead of the optimal $O(\log(1/\delta))$ that we're hoping
  for.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{References}

\begin{frame}[allowframebreaks]{References}
\nocite{*}
\bibliographystyle{amsalpha}
\bibliography{slides-refs}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
