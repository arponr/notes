\section{Applications}

\subsection{Markov matrices}

A matrix $\mA$ is a \textbf{Markov matrix} if all entries are $\ge 0$ and the sum of the entires in any one column is equal to 1. And actually the powers of any Markov matrix are also Markov matrices. An example would be
\[ \mA = \mat{.1&.01&.3\\.2&.99&.3\\.1&0&.4}. \]

Just like we say steady states in differential equations, we'll also see them in powers of Markov matrices. In differential equations we wanted one zero eigenvalue and the rest negative, because then an exponential would result in a constant function as time went on to infinity. But in powers, we want an eigenvalue of 1 to get a constant.

Now for some reason, it turns out that it is guaranteed for all Markov matrices that one eigenvalue is equal to 1. Moreover, all other eigenvalues satisfy $|\lambda| \le 1$. (The equal to happens only in those exceptional cases when the eigenvalue of 1 has multiplicity greater than one.)

We know that when we consider the sequence $\vu_{k+1}=\mA \vu_k$, then we have that in general
\[ \vu_k = \mA^k \vu_0 = \sum_{i=1}^n c_i \lambda_i^k \vx_i, \]
under the assumption that the eigenvectors are independent (allowing us to write $\vu_0$ as a linear combination of them). Now if $\mA$ is Markov (and let $\lambda_1 = 1$, and all other lambda have absolute value less than 1), then the steady state as $k \to \infty$ is just 
\[ c_1 \vx_1. \]

Now there ends up being something special with the eigenvector $\vx_1$ as well: its non-negative, and since the initial condition is non-negative, the steady state is then also non-negative. 

OK, now let's go back to that example, and see why 1 should be an eigenvalue. If we look at the matrix $\mA - \lambda \mI$ with $\lambda = 1$, we have
\[ \mA - \mI = \mat{-.9&.01&.3\\.2&-.01&.3\\.7&0&-.6}. \]
Notice that now all the columns must add to 0. And this implies that $\mA - \mI$ is singular, because the rows are dependent (simply because the sum of the rows must be 0, i.e. $\rvec{1,1,1}$ is in the left nullspace of $\mA - \mI$). So then there must be a vector in the nullspace of $\mA-\mI$, the eigenvector $\vx_1$. 

If we look at the last row, we can seet that the first entry of the eigenvector should by .6 and the third should be .1, which leaves the second 33, looking at the first or second rows. So 
\[ \vx_1 = \mat{.6\\33\\.7}. \]
And notice that indeed the entries are all non-negative. 

\brm
The eigenvalues of $\mA$ and $\T\mA$ are actually the same! For any eigenvalue of $\lambda$,
\[ \Det (\mA - \lambda \mI) = 0. \]
Then since the determinant is invariant under transposition, 
\[ \Det (\T{(\mA - \lambda \mI)}) = \Det (\T\mA - \lambda \mI) = 0. \]
So then $\lambda$ is an eigenvalue of $\T\mA$. (But the eigenvectors are different, because the nullspace and left nullspace of a matrix are different.)
\erm

So that's the linear algebra. But what's the application of these Markov matrices? We're solving and studying the equation
\[ \vu_{k+1} = \mA \vu_k, \]
for a Markov matrix $\mA$. Let's take a $2 \by 2$ example, where we have two states---say, California and Massachussetts---and we're looking at the population in each state. And the entires of our matrix $\mA_{ij}$ is going to tell us the fractions of the population that move from state $i$ to state $j$ (and this matrix stays constant over time). It's easy to see that this type of matrix satisfies the properties of a Markov matrix. So then we have, for instance,
\[ \mat{u_{cal}\\u_{mass}}_{k+1} = \mat{.9&.2\\.1&.8}\mat{u_{cal}\\u_{mass}}_{k}. \]
So 90\% of Californians stay there and 80\% of Massachussetts-ers stay there. The others move between the two states. We want to look at how the population distribution is going to change after a long period of time. 

Let's set 
\[ \mat{u_{cal}\\u_{mass}}_0 = \mat{0\\1000}. \]
So we know that at all times $u_{cal} + u_{mass} = 1000$. For example, after one step, we'll have 
\[ \mat{u_{cal}\\u_{mass}}_1 = \mat{200\\800}. \]
But if we want to know about later times, we need our eigenstuff for $\mA$.

We know one eigenvalue must be $\lambda_1 = 1$, and since the sum of the eigenvalues must be the trace $.9 + .8 = 1.7$, the other eigenvalue must be $\lambda_2 = .7$---notice that it is $<1$ as expected. Now for the first eigenvector we want to find a vector in the nullspace of 
\[ \mA - \mI = \mat{-.1&.2\\.1&-.2}. \]
It's easy to see that $\vx_1 = \rvec{2,1}$ (which is indeed non-negative).

Now from this alone we know the steady state. 2/3 of the thousand people will be in California and 1/3 will be in Massachussetts. But if we want only finite number of times steps, we need the other eigenvector, in the nullspace of
\[ \mA - .7\mI = \mat{.2&.2\\.1&.1}, \]
which we can see to be $\vx_2 = \rvec{-1,1}$. 

So then we know that
\[ \vu_k = c_1(1)^k\mat{2\\1} + c_2(.7)^k\mat{-1\\1}. \]
Given our initial condition, we know that
\[ \vu_0 = \mat{0\\1000} = c_1\mat{2\\1} + c_2\mat{-1\\1}. \]
We've already figured out that $c_1 = 1000/3$, and then it follows that $c_2 = 2000/3$. And that's the solution. 

\subsection{Projections and Fourier series}

Let's revisit projections in $\RR^n$ with an orthonormal basis, $\vq_1, \cdots, \vq_n$. Then we know for any vector $v \in \RR^n$,
\[ \vv = \sum_{i=1}^n x_i \vq_i. \]
Since the basis vectors are orthonormal, it's clear that 
\[ x_i = \T{\vq_i} \vv. \]
We can write this in matrix language as well. So we know that
\[ \mat{\vq_1 \cdots \vq_n} \mat{x_1\\\vdots\\x_n} = \vv. \]
Or, $\mQ \vx = \vv$. So,
\[ \vx = \mQ^{-1} \vv. \]
But since $\mQ$ is orthogonal, $\mQ^{-1} = \T\mQ$, so
\[ \vx = \T\mQ \vv, \]
which says the exact same thing as above.

This idea is what Fourier series are built on. Say we have a function $f$, and we want to write it in the form
\[ f(x) =  \sum_{i=0}^\infty a_n \cos (nx) + b_n \sin (nx) = a_0 + a_1 \cos x + b_1 \sin x + a_2 \cos 2x + b_2 \sin 2x + \cdots. \]
(This is called a \textbf{Fourier series}.) And now we're working in function space: our ``vectors'' are functions, and our basis must be infinite because the space is infinite-dimensional. So the basis for Fourier series is comprised of cosine and sine terms, as above. But why are these functions orthonormal? What does that mean? What is the inner product of $\cos x$ and $\sin x$ or $\cos x$ and $\cos 2x$, and why are all of these 0? For our regular vectors, our inner product was 
\[ \T\vv \vw = v_1w_1 + v_2w_2 + \cdots \]
But for functions we don't have finite components, we have infinitely many points. So what could the parallel idea of an inner product be? We'll it turns out the inner product in function space is
\[ \T f g = \int f(x)g(x)\,dx. \]
But we need limits too. In the case of Fourier series, all of the functions have a period $2 \pi$, so our range of integration is $[0,2\pi]$. Then for example,
\[ \int_0^{2\pi} \sin x \cos x\, dx = \frac 12 (\sin x)^2 \bigg|_0^{2\pi} = 0, \]
and we can check that the inner product is in fact 0 for all pairs of basis vectors in the Fourier series. 

So now we've got a new vector space to do projections with. Then to get, say, $a_1$, we take
\[ \int_0^{2\pi} f(x)\cos x \, dx. \]
But if we use the infinite series expression for $f$, then we see that all but the $a_1$ terms drop out (because the basis vectors are orthonormal), leaving us with
\[ a_1 \int_0^{2\pi} (\cos x)^2 \, dx = \int_0^{2\pi} f(x)\cos x \, dx, \]
which we can simplify to
\[ a_1 = \frac 1 \pi \int_0^{2\pi} f(x)\cos x \, dx. \]
And this is the idea of expansion into Fourier series.